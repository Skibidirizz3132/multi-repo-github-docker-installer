{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b24200c",
   "metadata": {},
   "source": [
    "#### Given a list of Git repositories, create containers and install dependencies for them - Python 3.11 recommended for conda env to run this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33741006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
    "\n",
    "# Get the OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb53b8",
   "metadata": {},
   "source": [
    "Extract github links from bookmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3364b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the path to the bookmarks HTML file on the Desktop\n",
    "bookmarks_file_name = 'bookmarks_6_3_25.html'\n",
    "desktop_path = os.path.expanduser('~/Desktop')\n",
    "bookmarks_file_path = os.path.join(desktop_path, bookmarks_file_name)\n",
    "\n",
    "github_links = []\n",
    "\n",
    "try:\n",
    "    with open(bookmarks_file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the Tutorials folder\n",
    "    tutorials_folder = soup.find('h3', string='Tutorials')\n",
    "    \n",
    "    if tutorials_folder:\n",
    "        # Get all links within the Tutorials folder's parent DT tag\n",
    "        tutorial_links = tutorials_folder.find_parent('dt').find_all('a')\n",
    "        \n",
    "        # Filter for GitHub links\n",
    "        for link_tag in tutorial_links:\n",
    "            href = link_tag.get('href')\n",
    "            if href and '/github.com' in href:\n",
    "                # Split URL by '/' and keep only up to owner/repo\n",
    "                parts = href.split('/')\n",
    "                if len(parts) > 5:  # github.com/owner/repo would be 5 parts\n",
    "                    cleaned_url = '/'.join(parts[:5])\n",
    "                else:\n",
    "                    cleaned_url = href\n",
    "                github_links.append(cleaned_url)\n",
    "\n",
    "        print(f\"Found {len(github_links)} GitHub tutorial links:\")\n",
    "        for link in github_links:\n",
    "            print(link)\n",
    "    else:\n",
    "        print(\"Tutorials folder not found in bookmarks\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{bookmarks_file_path}' was not found.\")\n",
    "    print(\"Please ensure the bookmarks file is on your Desktop and the name is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d296aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_links = list(dict.fromkeys(github_links))\n",
    "print(f\"{len(github_links)} unique links found\")\n",
    "\n",
    "github_links_subset=github_links[96:] #gpt4.1 costs ~14cents for 20 repos (83cents for ~100 repos), gpt4omini costs ~2cents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc691aee",
   "metadata": {},
   "source": [
    "Run only git links in folder 'Tutorials' in bookmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d186fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag # Added Tag for type checking\n",
    "\n",
    "def extract_links_from_tutorials(html_content):\n",
    "    \"\"\"\n",
    "    Extracts links from the 'Tutorials' folder in a bookmarks HTML file,\n",
    "    excluding links within subfolders of 'Tutorials'.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): The HTML content of the bookmarks file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains\n",
    "              the 'text' and 'href' of a bookmark.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    links = []\n",
    "\n",
    "    # Find the H3 tag for the \"Tutorials\" folder\n",
    "    tutorials_folder_h3 = None\n",
    "    for h3_tag_candidate in soup.find_all('h3'):\n",
    "        if h3_tag_candidate.get_text(strip=True).lower() == 'tutorials': # Case-insensitive match\n",
    "            tutorials_folder_h3 = h3_tag_candidate\n",
    "            break\n",
    "\n",
    "    if not tutorials_folder_h3:\n",
    "        print(\"Could not find a 'Tutorials' folder.\")\n",
    "        return links\n",
    "\n",
    "    # The list of bookmarks and subfolders is usually in the next <DL> tag\n",
    "    dl_tag = tutorials_folder_h3.find_next_sibling('dl')\n",
    "\n",
    "    if not dl_tag:\n",
    "        print(\"Could not find the <DL> list associated with the 'Tutorials' folder.\")\n",
    "        return links\n",
    "\n",
    "    # Iterate through each direct child of the folder's <DL> tag.\n",
    "    # A direct child might be a <P> tag (which then contains DTs/DLs),\n",
    "    # or less commonly, <DT> tags directly.\n",
    "    for direct_child_of_dl in dl_tag.children:\n",
    "        if not isinstance(direct_child_of_dl, Tag):\n",
    "            continue # Skip NavigableStrings like newlines\n",
    "\n",
    "        items_to_scan_in_block = []\n",
    "        if direct_child_of_dl.name == 'p':\n",
    "            # If the direct child of <DL> is a <P>, its children are the actual items (DTs, DLs)\n",
    "            items_to_scan_in_block = direct_child_of_dl.children\n",
    "        elif direct_child_of_dl.name == 'dt':\n",
    "            # If the direct child of <DL> is a <DT>, process it directly.\n",
    "            items_to_scan_in_block = [direct_child_of_dl]\n",
    "        # If direct_child_of_dl is another <DL> directly under the Tutorials <DL>\n",
    "        # (e.g., a subfolder's content list not wrapped in a <DT><H3> first),\n",
    "        # items_to_scan_in_block will remain empty, and the inner loop is skipped for it.\n",
    "        # print(items_to_scan_in_block)\n",
    "        for element in items_to_scan_in_block:\n",
    "            if not isinstance(element, Tag):\n",
    "                continue # Skip NavigableStrings\n",
    "            # We are interested in <DT> elements at this level\n",
    "            if element.name == 'dt':\n",
    "                def extract_top_level_links_with_names(tag):\n",
    "                    def walk(node, depth):\n",
    "                        if isinstance(node, Tag):\n",
    "                            if node.name == 'a' and depth == 0:\n",
    "                                links.append((node.get_text(strip=True), node['href']))\n",
    "                            new_depth = depth + 1 if node.name == 'dl' else depth\n",
    "                            for child in node.children:\n",
    "                                walk(child, new_depth)\n",
    "\n",
    "                    links = []\n",
    "                    walk(tag, 0)\n",
    "                    return links\n",
    "\n",
    "                links = extract_top_level_links_with_names(element)\n",
    "                # print(links)\n",
    "\n",
    "    return links\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Load the HTML content from your file\n",
    "file_path = 'bookmarks_6_3_25.html' # Make sure this is your file\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f: #\n",
    "        html_doc = f.read() #\n",
    "\n",
    "    # Extract the links\n",
    "    tutorial_links = extract_links_from_tutorials(html_doc)\n",
    "\n",
    "    # Print the extracted links\n",
    "    if tutorial_links:\n",
    "        print(\"\\nLinks found directly in 'Tutorials' folder (excluding subfolders):\")\n",
    "        main_links = []\n",
    "        main_names = []\n",
    "        for name, link in tutorial_links:\n",
    "            print(f\"{name}: {link}\")\n",
    "            main_links.append(link)\n",
    "            main_names.append(name)\n",
    "\n",
    "    else:\n",
    "        print(\"No links were found directly under the 'Tutorials' folder, or the folder itself was not found.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d717b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before filtering, {len(main_links)} links were found\")\n",
    "main_links = [item for item in main_links if 'github' in item]\n",
    "print(f\"After filtering, {len(main_links)} links containing 'github' in their URL remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a68966",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_links_subset = main_links[:100]#[:len(main_links)//2]  # Get first half of the links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe820b3",
   "metadata": {},
   "source": [
    "### Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583af52",
   "metadata": {},
   "source": [
    "Sequential run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509967c",
   "metadata": {},
   "source": [
    "Only for pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# import json\n",
    "# import openai, subprocess, os, re\n",
    "# from gitingest import ingest\n",
    "# import docker\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "# import time\n",
    "\n",
    "# # Initialize Docker client\n",
    "# docker_client = docker.from_env()\n",
    "\n",
    "# # Global tracking dictionaries\n",
    "# error_log = {}  # repo_name -> error_string\n",
    "# requirements_successful = {}  # repo_name -> container_id\n",
    "# code_successful = {}  # repo_name -> container_id\n",
    "# requirements_successful_code_failed = {}  # repo_name -> container_id\n",
    "\n",
    "# async def setup_repo(git_url, base_dir='repos'):\n",
    "#     repo_name = git_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "#     print(\"Repo name is:\", repo_name)\n",
    "#     repo_dir = Path(base_dir) / repo_name\n",
    "#     # print(\"Repo dir is:\", repo_dir)\n",
    "#     if not repo_dir.exists():\n",
    "#         # Run blocking subprocess in a thread\n",
    "#         await asyncio.to_thread(subprocess.run, ['git', 'clone', git_url, str(repo_dir)], check=True)\n",
    "#     return repo_dir\n",
    "\n",
    "# async def ingest_repo(repo_dir): # Made async\n",
    "#     # Run blocking ingest call in a thread\n",
    "#     # Changed include_patterns to be a set of strings as expected by gitingest\n",
    "#     summary, tree, content = await asyncio.to_thread(ingest, str(repo_dir), include_patterns={\"README.md\", \"requirements.txt\"})\n",
    "#     return summary, tree, content\n",
    "\n",
    "# async def extract_main_code(documents): # Made async\n",
    "#     prompt = (\n",
    "#         \"Given the following content from a GitHub repo, extract or synthesize a minimal executable .py script \"\n",
    "#         \"that demonstrates how to use the package or run its main functionality.\"\n",
    "#         \"The output should be valid Python code for a `test_code.py` file. If there's nothing executable, just output \"\n",
    "#         \"`print(\\\"no code found\\\")`. The content is: \"\n",
    "#         f\"{documents} \\n \\n\"\n",
    "#     )\n",
    "#     # Use async version of OpenAI call\n",
    "#     response = await openai.ChatCompletion.acreate( # Changed to acreate\n",
    "#         model=\"gpt-4.1-2025-04-14\", #\"gpt-4o-mini-2024-07-18\", #\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         temperature=0.0, \n",
    "#         api_key=openai_api_key,\n",
    "#     )\n",
    "#     code_blocks = response.choices[0].message.content\n",
    "#     # print(\"[i] Extracted code blocks:\", code_blocks)\n",
    "#     return extract_first_code_block(code_blocks) or 'print(\"no code found\")'\n",
    "\n",
    "# def extract_first_code_block(text):\n",
    "#     m = re.search(r\"```(?:python)?\\s*(.*?)```\", text, re.DOTALL)\n",
    "#     return m.group(1).strip() if m else text.strip()\n",
    "\n",
    "# def write_test_code(repo_dir, code):\n",
    "#     path = Path(repo_dir) / 'test_code.py'\n",
    "#     with open(path, 'w') as f:\n",
    "#         f.write(code)\n",
    "#     return path\n",
    "\n",
    "# def parse_requirements_txt(doc):\n",
    "#     # Extract requirements.txt content if present\n",
    "#     # Assuming 'doc' is a string containing the content of all ingested files,\n",
    "#     # and each file's content is prefixed by its name.\n",
    "#     # A more robust way would be if `ingest` returned structured data.\n",
    "#     # For now, we search for a section starting with \"File: requirements.txt\" or similar.\n",
    "#     # If `ingest` concatenates files, we need a reliable delimiter.\n",
    "#     # Let's assume `gitingest` provides file content distinctly or `doc` contains identifiable sections.\n",
    "#     # A simple regex for \"requirements.txt\" content block:\n",
    "#     req_content = \"\"\n",
    "#     # Try to find requirements.txt content. This might need adjustment based on `ingest` output format.\n",
    "#     # If `docs` is a dictionary mapping filenames to content:\n",
    "#     if isinstance(doc, dict) and \"requirements.txt\" in doc:\n",
    "#         req_content = doc[\"requirements.txt\"]\n",
    "#     # If `docs` is a single string with file contents concatenated:\n",
    "#     else:\n",
    "#         # This regex assumes a simple structure like \"File: requirements.txt\\n<content>\"\n",
    "#         # or just the content of requirements.txt if it's the only thing related to it.\n",
    "#         # This part is speculative without knowing the exact format of `docs` from `ingest`\n",
    "#         # when multiple patterns are used.\n",
    "#         # For gitingest, `content` is a string where files are concatenated,\n",
    "#         # prefixed by \"File: <filepath>\\n---\\n<content>\\n---\".\n",
    "#         match = re.search(r\"File:.*?requirements\\.txt\\n---\\n(.*?)\\n---\", doc, re.DOTALL | re.IGNORECASE)\n",
    "#         if match:\n",
    "#             req_content = match.group(1)\n",
    "\n",
    "#     requirements = []\n",
    "#     if req_content:\n",
    "#         for line in req_content.split('\\n'):\n",
    "#             line = line.strip()\n",
    "#             if line and not line.startswith('#'):\n",
    "#                 # Remove version specifiers, comments, and extras\n",
    "#                 package = re.split(r'[=<>!~#\\[]', line)[0].strip()\n",
    "#                 if package:\n",
    "#                     requirements.append(package)\n",
    "#     return requirements\n",
    "\n",
    "# async def install_all_dependencies(container, doc, repo_dir=\"/app\", repo_name=\"\"):\n",
    "#     # First get dependencies from requirements.txt if present in ingested docs\n",
    "#     parsed_deps = parse_requirements_txt(doc)\n",
    "    \n",
    "#     # Then get additional dependencies from content analysis using LLM\n",
    "#     prompt = (\n",
    "#         \"Based on the content of the file below (primarily README.md, but also consider other context if provided), \"\n",
    "#         \"create a list of Python packages to be added in a requirements.txt file. \"\n",
    "#         \"List only the package names, one per line. Only include packages explicitly mentioned for installation or clearly imported and used. \"\n",
    "#         \"Do not include any dashes or version numbers. The content of the file is: \" + doc\n",
    "#     )\n",
    "#     # Use async version of OpenAI call\n",
    "#     response = await openai.ChatCompletion.acreate( # Changed to acreate\n",
    "#         model=\"gpt-4.1-2025-04-14\", #\"gpt-4o-mini-2024-07-18\", #\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         temperature=0.0,\n",
    "#         api_key=openai_api_key,\n",
    "#     )\n",
    "    \n",
    "#     llm_deps = [dep.strip() for dep in response.choices[0].message.content.strip().split(\"\\n\") if dep.strip() and not ' ' in dep.strip()]\n",
    "    \n",
    "#     # Combine both sources of dependencies and remove duplicates\n",
    "#     dependencies = list(dict.fromkeys(parsed_deps + llm_deps))\n",
    "#     requirements_content = \"\\n\".join(dependencies)\n",
    "    \n",
    "#     if not requirements_content.strip():\n",
    "#         print(\"[i] No dependencies found to install.\")\n",
    "#         requirements_successful[repo_name] = container.id\n",
    "#         return True\n",
    "\n",
    "#     # Run blocking docker exec_run in a thread\n",
    "#     await asyncio.to_thread(container.exec_run, f\"bash -c 'echo \\\"{requirements_content}\\\" > {repo_dir}/requirements.txt'\")\n",
    "#     print(f\"[i] Created requirements.txt in {container.name} with contents:\\n{requirements_content}\")\n",
    "    \n",
    "#     # Run blocking docker exec_run in a thread\n",
    "#     result = await asyncio.to_thread(container.exec_run, f\"pip install -r requirements.txt\", workdir=repo_dir)\n",
    "#     print(f\"[i] Installing dependencies from requirements.txt in {repo_dir}\")\n",
    "#     print(f\"[i] Output for {repo_dir}:\", result.output.decode()[:200], '\\n \\nENDING WITH:', result.output.decode()[-400:]) #only print first 200 characters\n",
    "    \n",
    "#     # Check if requirements installation was successful\n",
    "#     if result.exit_code == 0:\n",
    "#         requirements_successful[repo_name] = container.id\n",
    "#         return True\n",
    "#     else:\n",
    "#         error_message = f\"Requirements installation failed: {result.output.decode()}\"\n",
    "#         error_log[repo_name] = error_message\n",
    "#         print(f\"[!] Requirements installation failed for {repo_name}: {error_message}\")\n",
    "#         return False\n",
    "\n",
    "# async def extract_missing_dependencies(traceback_text):\n",
    "#     \"\"\"Extract missing dependencies from traceback using OpenAI\"\"\"\n",
    "#     prompt = (\n",
    "#         \"Given the following Python traceback error, identify any missing Python packages that need to be installed. \"\n",
    "#         \"Return only the package names, one per line, without any version numbers or additional text. \"\n",
    "#         \"If no missing packages can be identified, return an empty response. \"\n",
    "#         \"The traceback is: \" + traceback_text\n",
    "#     )\n",
    "    \n",
    "#     response = await openai.ChatCompletion.acreate(\n",
    "#         model=\"gpt-4.1-2025-04-14\", #\"gpt-4o-mini-2024-07-18\", #\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         temperature=0.0,\n",
    "#         api_key=openai_api_key,\n",
    "#     )\n",
    "    \n",
    "#     missing_deps = [dep.strip() for dep in response.choices[0].message.content.strip().split(\"\\n\") if dep.strip() and not ' ' in dep.strip()]\n",
    "#     return missing_deps\n",
    "\n",
    "# async def install_missing_dependencies(container, missing_deps, repo_dir=\"/app\"):\n",
    "#     \"\"\"Install missing dependencies identified from traceback\"\"\"\n",
    "#     if not missing_deps:\n",
    "#         return False\n",
    "    \n",
    "#     # Read current requirements.txt\n",
    "#     result = await asyncio.to_thread(container.exec_run, f\"cat {repo_dir}/requirements.txt\", workdir=repo_dir)\n",
    "#     current_requirements = result.output.decode().strip().split('\\n') if result.exit_code == 0 else []\n",
    "    \n",
    "#     # Add missing dependencies that aren't already in requirements\n",
    "#     new_deps = [dep for dep in missing_deps if dep not in current_requirements]\n",
    "    \n",
    "#     if new_deps:\n",
    "#         all_requirements = current_requirements + new_deps\n",
    "#         requirements_content = \"\\n\".join(filter(None, all_requirements))  # Filter out empty strings\n",
    "        \n",
    "#         # Update requirements.txt\n",
    "#         await asyncio.to_thread(container.exec_run, f\"bash -c 'echo \\\"{requirements_content}\\\" > {repo_dir}/requirements.txt'\")\n",
    "#         print(f\"[i] Updated requirements.txt with missing dependencies: {new_deps}\")\n",
    "#         print(\"Full requirements.txt content:\", requirements_content)\n",
    "        \n",
    "#         # Install the new dependencies\n",
    "#         for dep in new_deps:\n",
    "#             result = await asyncio.to_thread(container.exec_run, f\"pip install {dep}\", workdir=repo_dir)\n",
    "#             print(f\"[i] Installing {dep}: {result.output.decode()}\")\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# async def run_test_code(container, repo_dir=\"/app\", repo_name=\"\"):\n",
    "#     # Run blocking docker exec_run in a thread\n",
    "#     result = await asyncio.to_thread(container.exec_run, \"python test_code.py\", workdir=repo_dir)\n",
    "#     output_str = result.output.decode()\n",
    "#     print(f\"[o] Output for {repo_dir} ({container.name}):\\n\", output_str)\n",
    "\n",
    "#     if \"Traceback (most recent call last):\" in output_str:#output_str.strip().startswith(\"Traceback\"):\n",
    "#         print(f\"[!] Traceback detected in {container.name}. Attempting to fix missing dependencies.\")\n",
    "        \n",
    "#         # Track that requirements were successful but code initially failed\n",
    "#         if repo_name in requirements_successful:\n",
    "#             requirements_successful_code_failed[repo_name] = container.id\n",
    "        \n",
    "#         # Try up to 2 times to install missing dependencies\n",
    "#         dependency_install_successful = False\n",
    "#         for attempt in range(2):\n",
    "#             print(f\"[i] Dependency installation attempt {attempt + 1}/2\")\n",
    "            \n",
    "#             # Extract missing dependencies from the traceback\n",
    "#             missing_deps = await extract_missing_dependencies(output_str)\n",
    "            \n",
    "#             if missing_deps:\n",
    "#                 print(f\"[i] Found potential missing dependencies: {missing_deps}\")\n",
    "#                 install_success = await install_missing_dependencies(container, missing_deps, repo_dir)\n",
    "                \n",
    "#                 if install_success:\n",
    "#                     dependency_install_successful = True\n",
    "#                     # Retry running the test code\n",
    "#                     print(f\"[i] Retrying test code execution in {container.name}\")\n",
    "#                     retry_result = await asyncio.to_thread(container.exec_run, \"python test_code.py\", workdir=repo_dir)\n",
    "#                     retry_output = retry_result.output.decode()\n",
    "#                     print(f\"[o] Retry output for ({container.name}):\\n\", retry_output)\n",
    "                    \n",
    "#                     # Check if retry was successful\n",
    "#                     if not \"Traceback\" in retry_output:#retry_output.strip().startswith(\"Traceback\"):\n",
    "#                         print(f\"[✓] Test code succeeded after installing missing dependencies in {container.name}\")\n",
    "#                         code_successful[repo_name] = container.id\n",
    "#                         # Remove from failed dict if it was there\n",
    "#                         if repo_name in requirements_successful_code_failed:\n",
    "#                             del requirements_successful_code_failed[repo_name]\n",
    "#                         return True\n",
    "#                     elif 'ModuleNotFoundError' not in retry_output:\n",
    "#                         print(f\"[!] Dependencies installed successfully in {container.name} but other error occurred\")\n",
    "#                         error_log[repo_name] = f\"Code execution error after dependency installation: {retry_output}\"\n",
    "#                         return False\n",
    "#                     else:\n",
    "#                         # Update output_str for next iteration if there's another attempt\n",
    "#                         output_str = retry_output\n",
    "#             else:\n",
    "#                 print(f\"[!] No missing dependencies identified from traceback on attempt {attempt + 1}.\")\n",
    "#                 break  # No deps found, don't continue trying\n",
    "        \n",
    "#         # If we've exhausted attempts and still have dependency issues, delete container\n",
    "#         if dependency_install_successful and output_str.strip().startswith(\"Traceback\") and 'ModuleNotFoundError' in output_str:\n",
    "#             print(f\"[!] Unable to resolve dependencies after 2 attempts. Deleting container {container.name}.\")\n",
    "#             error_log[repo_name] = f\"Unresolvable dependency issues after 2 attempts: {output_str}\"\n",
    "#             try:\n",
    "#                 await asyncio.to_thread(container.stop)\n",
    "#                 await asyncio.to_thread(container.remove)\n",
    "#                 print(f\"[!] Deleted container {container.name} due to unresolvable dependency issues.\")\n",
    "#             except docker.errors.APIError as e:\n",
    "#                 print(f\"[!] Error stopping/removing container {container.name}: {e}\")\n",
    "#             return False\n",
    "#         elif not dependency_install_successful:\n",
    "#             error_log[repo_name] = f\"No dependencies could be extracted from traceback: {output_str}\"\n",
    "#             print(f\"[!] No dependencies could be extracted. Keeping container {container.name} for manual inspection.\")\n",
    "#             return False\n",
    "#         else:\n",
    "#             error_log[repo_name] = f\"Code execution error (non-dependency): {output_str}\"\n",
    "#             print(f\"[!] Non-dependency error in {container.name}. Keeping container for inspection.\")\n",
    "#             return False\n",
    "            \n",
    "#     elif output_str.strip().startswith(\"File not found\"):\n",
    "#         print(f\"[!] File not found error in {container.name}. Keeping container for inspection.\")\n",
    "#         error_log[repo_name] = f\"File not found error: {output_str}\"\n",
    "#         return False\n",
    "    \n",
    "#     # Test code succeeded\n",
    "#     code_successful[repo_name] = container.id\n",
    "#     # Remove from failed dict if it was there\n",
    "#     if repo_name in requirements_successful_code_failed:\n",
    "#         del requirements_successful_code_failed[repo_name]\n",
    "#     return True\n",
    "\n",
    "# async def full_pipeline(git_url):\n",
    "#     repo_name = git_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    \n",
    "#     try:\n",
    "#         repo_dir = await setup_repo(git_url)\n",
    "#         summary, tree, docs = await ingest_repo(repo_dir) # Added await\n",
    "#         # print(f\"[i] Ingested documents for {git_url}:\\n{docs}\\n\") # For debugging doc format\n",
    "#         # print(\"[i] Summary:\", summary)\n",
    "#         # print(\"[i] Tree:\", tree)\n",
    "#         code = await extract_main_code(docs) # Added await\n",
    "        \n",
    "#         # If no code found, return early without creating container\n",
    "#         if code.strip() == 'print(\"no code found\")':\n",
    "#             print(f\"No usable code found for {git_url}, skipping container creation\")\n",
    "#             error_log[repo_name] = \"No usable code found\"\n",
    "#             return None\n",
    "            \n",
    "#         write_test_code(repo_dir, code) # Sync, assumed fast\n",
    "        \n",
    "#         container_name = f\"repo_{repo_name}\"\n",
    "        \n",
    "#         # Check if container already exists and delete it\n",
    "#         try:\n",
    "#             existing_container = await asyncio.to_thread(docker_client.containers.get, container_name)\n",
    "#             if existing_container:\n",
    "#                 print(f\"Found existing container with name: {container_name}. Deleting it.\")\n",
    "#                 await asyncio.to_thread(existing_container.stop)\n",
    "#                 await asyncio.to_thread(existing_container.remove)\n",
    "#                 print(f\"Deleted container with name: {container_name}\")\n",
    "#         except docker.errors.NotFound:\n",
    "#             print(f\"No existing container found with name: {container_name}. Proceeding to create a new one.\")\n",
    "#         except docker.errors.APIError as e:\n",
    "#             print(f\"Error checking for existing container {container_name}: {e}\")\n",
    "#             # Decide if you want to stop or continue. For now, let's try to continue.\n",
    "#             pass\n",
    "\n",
    "#         # Run blocking docker containers.run in a thread using Linux-based Python image\n",
    "#         container = await asyncio.to_thread(\n",
    "#             docker_client.containers.run,\n",
    "#             \"python:3.10-slim\",\n",
    "#             name=container_name,\n",
    "#             detach=True,\n",
    "#             tty=True,\n",
    "#             volumes={str(repo_dir.absolute()): {'bind': '/app', 'mode': 'rw'}},\n",
    "#             working_dir='/app',\n",
    "#             platform='linux/amd64' # Specify platform for consistency\n",
    "#         )\n",
    "        \n",
    "#         print(f\"[i] Created Docker container: {container_name}\")\n",
    "        \n",
    "#         container_info = {\n",
    "#             \"repo\": repo_name,\n",
    "#             \"container_id\": container.id,\n",
    "#             \"container_name\": container_name\n",
    "#         }\n",
    "        \n",
    "#         # Save json inside repo directory\n",
    "#         with open(repo_dir / f\"container_{repo_name}.json\", \"w\") as f:\n",
    "#             json.dump(container_info, f)\n",
    "\n",
    "#         # Install dependencies only after container is confirmed running\n",
    "#         requirements_success = await install_all_dependencies(container, docs, \"/app\", repo_name)\n",
    "        \n",
    "#         if not requirements_success:\n",
    "#             print(f\"Requirements installation failed for {git_url}. Keeping container {container_name} for inspection.\")\n",
    "#             return container\n",
    "        \n",
    "#         test_successful = await run_test_code(container, \"/app\", repo_name)\n",
    "        \n",
    "#         return container\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         error_log[repo_name] = f\"Pipeline error: {str(e)}\"\n",
    "#         print(f\"Pipeline error for {git_url}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# async def run_all_pipelines():\n",
    "#     # Ensure openai_api_key is set before running, e.g.\n",
    "#     # openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "#     # Or ensure it's globally available if not passed explicitly to functions.\n",
    "#     # For this example, I've modified extract_main_code and install_all_dependencies\n",
    "#     # to use os.getenv(\"OPENAI_API_KEY\") directly.\n",
    "\n",
    "#     # Run pipelines sequentially, one at a time\n",
    "#     successful_containers = []\n",
    "#     container_ids = {}\n",
    "\n",
    "#     Path('repos').mkdir(exist_ok=True)\n",
    "\n",
    "#     for i, url in enumerate(tqdm(github_links_subset, desc=\"Processing repositories\")):\n",
    "#         time.sleep(10)\n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(f\"Processing repository: {url}\")\n",
    "#         print(f\"{'='*80}\")\n",
    "        \n",
    "#         try:\n",
    "#             result = await full_pipeline(url)\n",
    "#             repo_name_for_id = url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "            \n",
    "#             if isinstance(result, docker.models.containers.Container):\n",
    "#                 successful_containers.append(result)\n",
    "#                 container_ids[repo_name_for_id] = result.id\n",
    "#                 print(f\"Pipeline for {url} completed. Container ID: {result.id}\")\n",
    "#             elif result is None:\n",
    "#                 # This case is handled if full_pipeline returns None (e.g. test code failed)\n",
    "#                 print(f\"Pipeline for {url} did not complete successfully or container was removed.\")\n",
    "#             else:\n",
    "#                 print(f\"Pipeline for {url} returned unexpected result: {result}\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             repo_name_for_id = url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "#             error_log[repo_name_for_id] = f\"Pipeline error: {str(e)}\"\n",
    "#             print(f\"Pipeline for {url} failed with an exception: {e}\")\n",
    "#             # Optionally, log the traceback:\n",
    "#             # import traceback\n",
    "#             # traceback.print_exception(type(e), e, e.__traceback__)\n",
    "\n",
    "#         # Save tracking information every 5 URLs\n",
    "#         if (i + 1) % 5 == 0 or i == len(github_links_subset) - 1:\n",
    "#             print(f\"\\nSaving tracking information after processing {i + 1} repositories...\")\n",
    "            \n",
    "#             with open(\"repos/all_container_ids.json\", \"w\") as f:\n",
    "#                 json.dump(container_ids, f)\n",
    "            \n",
    "#             with open(\"repos/error_log.json\", \"w\") as f:\n",
    "#                 json.dump(error_log, f, indent=2)\n",
    "            \n",
    "#             with open(\"repos/requirements_successful.json\", \"w\") as f:\n",
    "#                 json.dump(requirements_successful, f, indent=2)\n",
    "            \n",
    "#             with open(\"repos/code_successful.json\", \"w\") as f:\n",
    "#                 json.dump(code_successful, f, indent=2)\n",
    "            \n",
    "#             with open(\"repos/requirements_successful_code_failed.json\", \"w\") as f:\n",
    "#                 json.dump(requirements_successful_code_failed, f, indent=2)\n",
    "    \n",
    "#     print(f\"\\nFinal results:\")\n",
    "#     print(f\"Successfully processed {len(successful_containers)} repositories.\")\n",
    "#     print(f\"Requirements successful: {len(requirements_successful)} repositories\")\n",
    "#     print(f\"Code successful: {len(code_successful)} repositories\")\n",
    "#     print(f\"Requirements successful but code failed: {len(requirements_successful_code_failed)} repositories\")\n",
    "#     print(f\"Errors logged: {len(error_log)} repositories\")\n",
    "#     print(f\"Container IDs saved to repos/all_container_ids.json: {container_ids}\")\n",
    "#     print(\"Error log saved to repos/error_log.json\")\n",
    "#     print(\"Requirements successful saved to repos/requirements_successful.json\")\n",
    "#     print(\"Code successful saved to repos/code_successful.json\")\n",
    "#     print(\"Requirements successful but code failed saved to repos/requirements_successful_code_failed.json\")\n",
    "\n",
    "# # Run in Jupyter/IPython:\n",
    "# await run_all_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run till here 190 repos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831ed55",
   "metadata": {},
   "source": [
    "Pip and npm installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963fa0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import openai, subprocess, os, re\n",
    "from gitingest import ingest\n",
    "import docker\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize Docker client\n",
    "docker_client = docker.from_env()\n",
    "\n",
    "# Global tracking dictionaries\n",
    "error_log = {}  # repo_name -> error_string\n",
    "requirements_successful = {}  # repo_name -> container_id\n",
    "code_successful = {}  # repo_name -> container_id\n",
    "requirements_successful_code_failed = {}  # repo_name -> container_id\n",
    "\n",
    "# Assumed to be globally available or set via os.getenv(\"OPENAI_API_KEY\")\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\") # Example if needed\n",
    "\n",
    "def append_error(repo_name, error_type_prefix, message):\n",
    "    \"\"\"Helper function to append errors to the global error_log.\"\"\"\n",
    "    full_message = f\"{error_type_prefix}: {message}\"\n",
    "    existing_error = error_log.get(repo_name)\n",
    "    if existing_error:\n",
    "        error_log[repo_name] = f\"{existing_error}; {full_message}\"\n",
    "    else:\n",
    "        error_log[repo_name] = full_message\n",
    "    # Print a truncated version of the message to keep logs readable\n",
    "    print(f\"[!] {error_type_prefix} for {repo_name}: {message[:500]}{'...' if len(message) > 500 else ''}\")\n",
    "\n",
    "\n",
    "async def setup_repo(git_url, base_dir='repos'):\n",
    "    repo_name = git_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    print(\"Repo name is:\", repo_name)\n",
    "    repo_dir = Path(base_dir) / repo_name\n",
    "    # print(\"Repo dir is:\", repo_dir)\n",
    "    if not repo_dir.exists():\n",
    "        # Run blocking subprocess in a thread\n",
    "        await asyncio.to_thread(subprocess.run, ['git', 'clone', git_url, str(repo_dir)], check=True)\n",
    "    return repo_dir\n",
    "\n",
    "async def ingest_repo(repo_dir): # Made async\n",
    "    # Run blocking ingest call in a thread\n",
    "    # Added \"package.json\" to include_patterns\n",
    "    summary, tree, content = await asyncio.to_thread(\n",
    "        ingest, str(repo_dir), \n",
    "        include_patterns={\"README.md\", \"requirements.txt\", \"package.json\"}\n",
    "    )\n",
    "    return summary, tree, content\n",
    "\n",
    "async def extract_main_code(documents): # Made async\n",
    "    prompt = (\n",
    "        \"Given the following content from a GitHub repo, extract or synthesize a minimal executable .py script \"\n",
    "        \"that demonstrates how to use the package or run its main functionality.\"\n",
    "        \"The output should be valid Python code for a `test_code.py` file. If there's nothing executable, just output \"\n",
    "        \"`print(\\\"no code found\\\")`. The content is: \"\n",
    "        f\"{documents} \\n \\n\"\n",
    "    )\n",
    "    # Use async version of OpenAI call\n",
    "    response = await openai.ChatCompletion.acreate( # Changed to acreate\n",
    "        model=\"gpt-4.1-2025-04-14\", #\"gpt-4o-mini-2024-07-18\", #\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0, \n",
    "        api_key=openai_api_key, # Make sure openai_api_key is defined in the global scope or passed\n",
    "    )\n",
    "    code_blocks = response.choices[0].message.content\n",
    "    # print(\"[i] Extracted code blocks:\", code_blocks)\n",
    "    return extract_first_code_block(code_blocks) or 'print(\"no code found\")'\n",
    "\n",
    "def extract_first_code_block(text):\n",
    "    m = re.search(r\"```(?:python)?\\s*(.*?)```\", text, re.DOTALL)\n",
    "    return m.group(1).strip() if m else text.strip()\n",
    "\n",
    "def write_test_code(repo_dir, code):\n",
    "    path = Path(repo_dir) / 'test_code.py'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(code)\n",
    "    return path\n",
    "\n",
    "def parse_requirements_txt(doc):\n",
    "    req_content = \"\"\n",
    "    match = re.search(r\"File:.*?requirements\\.txt\\n---\\n(.*?)\\n---\", doc, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        req_content = match.group(1)\n",
    "\n",
    "    requirements = []\n",
    "    if req_content:\n",
    "        for line in req_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                package = re.split(r'[=<>!~#\\[]', line)[0].strip()\n",
    "                if package:\n",
    "                    requirements.append(package)\n",
    "    return requirements\n",
    "\n",
    "async def install_all_dependencies(container, doc, repo_dir=\"/app\", repo_name=\"\"):\n",
    "    overall_deps_successful = True\n",
    "    \n",
    "    # --- NPM Dependency Installation ---\n",
    "    has_package_json = False\n",
    "    # Check if package.json content was ingested\n",
    "    match_pkg_json = re.search(r\"File:.*?package\\.json\\n---\\n\", doc, re.IGNORECASE | re.DOTALL)\n",
    "    if match_pkg_json:\n",
    "        has_package_json = True\n",
    "        print(f\"[i] Found package.json content in ingested docs for {repo_name}.\")\n",
    "\n",
    "    if has_package_json:\n",
    "        print(f\"[i] Attempting npm install for {repo_name}.\")\n",
    "        \n",
    "        # Install Node.js and npm. python:slim images run as root.\n",
    "        print(f\"[i] Installing Node.js and npm in {container.name}...\")\n",
    "        # Using -qq for quieter output from apt-get\n",
    "        install_npm_cmd = \"apt-get update -qq && apt-get install -y -qq nodejs npm\"\n",
    "        \n",
    "        npm_setup_result = await asyncio.to_thread(container.exec_run, f\"bash -c '{install_npm_cmd}'\")\n",
    "        \n",
    "        if npm_setup_result.exit_code == 0:\n",
    "            print(f\"[i] Node.js and npm installed successfully in {container.name}.\")\n",
    "            \n",
    "            # Run npm install\n",
    "            npm_install_cmd = \"npm install\"\n",
    "            print(f\"[i] Running '{npm_install_cmd}' in {repo_dir} for {repo_name}...\")\n",
    "            npm_result = await asyncio.to_thread(container.exec_run, npm_install_cmd, workdir=repo_dir)\n",
    "            \n",
    "            npm_output = npm_result.output.decode()\n",
    "            print(f\"[i] npm install output for {repo_name} (first 200 chars):\\n{npm_output[:200]}\")\n",
    "            if len(npm_output) > 600: # Show tail if output is long\n",
    "                 print(f\"[i] npm install output for {repo_name} (last 400 chars):\\n{npm_output[-400:]}\")\n",
    "\n",
    "\n",
    "            if npm_result.exit_code != 0:\n",
    "                overall_deps_successful = False\n",
    "                append_error(repo_name, \"NPM_INSTALL_ERROR\", npm_output)\n",
    "            else:\n",
    "                print(f\"[✓] npm install successful for {repo_name}.\")\n",
    "        else:\n",
    "            overall_deps_successful = False\n",
    "            append_error(repo_name, \"NPM_SETUP_ERROR\", npm_setup_result.output.decode())\n",
    "    else:\n",
    "        print(f\"[i] No package.json found or ingested for {repo_name}. Skipping npm install.\")\n",
    "\n",
    "    # --- Python Dependency Installation ---\n",
    "    if not overall_deps_successful:\n",
    "        print(f\"[!] Skipping Python dependencies due to earlier npm failure for {repo_name}.\")\n",
    "    else:\n",
    "        parsed_deps = parse_requirements_txt(doc)\n",
    "        \n",
    "        prompt_python_deps = (\n",
    "            \"Based on the content of the file below (primarily README.md, but also consider other context if provided), \"\n",
    "            \"create a list of Python packages to be added in a requirements.txt file. \"\n",
    "            \"List only the package names, one per line. Only include packages explicitly mentioned for installation or clearly imported and used. \"\n",
    "            \"Do not include any dashes or version numbers. The content of the file is: \" + doc\n",
    "        )\n",
    "        response_python_deps = await openai.ChatCompletion.acreate(\n",
    "            model=\"gpt-4.1-2025-04-14\", #\"gpt-4o-mini-2024-07-18\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_python_deps}],\n",
    "            temperature=0.0,\n",
    "            api_key=openai_api_key, # Make sure openai_api_key is defined\n",
    "        )\n",
    "        llm_deps = [dep.strip() for dep in response_python_deps.choices[0].message.content.strip().split(\"\\n\") if dep.strip() and not ' ' in dep.strip()]\n",
    "        \n",
    "        dependencies = list(dict.fromkeys(parsed_deps + llm_deps))\n",
    "        requirements_content = \"\\n\".join(dependencies)\n",
    "        \n",
    "        if not requirements_content.strip():\n",
    "            print(f\"[i] No Python dependencies found to install for {repo_name}.\")\n",
    "        else:\n",
    "            await asyncio.to_thread(container.exec_run, f\"bash -c 'echo \\\"{requirements_content}\\\" > {repo_dir}/requirements.txt'\")\n",
    "            print(f\"[i] Created requirements.txt in {container.name} with contents:\\n{requirements_content}\")\n",
    "            \n",
    "            print(f\"[i] Installing Python dependencies from requirements.txt in {repo_dir} for {repo_name}...\")\n",
    "            pip_result = await asyncio.to_thread(container.exec_run, f\"pip install -r requirements.txt\", workdir=repo_dir)\n",
    "            \n",
    "            pip_output = pip_result.output.decode()\n",
    "            print(f\"[i] Pip install output for {repo_dir}:\", pip_output[:200], '\\n \\nENDING WITH:', pip_output[-400:])\n",
    "            \n",
    "            if pip_result.exit_code != 0:\n",
    "                overall_deps_successful = False\n",
    "                append_error(repo_name, \"PIP_INSTALL_ERROR\", pip_output)\n",
    "            else:\n",
    "                print(f\"[✓] Pip install successful for {repo_name}.\")\n",
    "\n",
    "    if overall_deps_successful:\n",
    "        requirements_successful[repo_name] = container.id\n",
    "        print(f\"[✓] All dependency installations successful for {repo_name}.\")\n",
    "    else:\n",
    "        if repo_name in requirements_successful: # Should not happen if logic is correct, but as safeguard\n",
    "            del requirements_successful[repo_name]\n",
    "        print(f\"[!] One or more dependency installation steps failed for {repo_name}.\")\n",
    "        # Specific error details are already in error_log via append_error\n",
    "\n",
    "    return overall_deps_successful\n",
    "\n",
    "async def extract_missing_dependencies(traceback_text):\n",
    "    \"\"\"Extract missing dependencies from traceback using OpenAI\"\"\"\n",
    "    prompt = (\n",
    "        \"Given the following Python traceback error, identify any missing Python packages that need to be installed. \"\n",
    "        \"Return only the package names, one per line, without any version numbers or additional text. \"\n",
    "        \"If no missing packages can be identified, return an empty response. \"\n",
    "        \"The traceback is: \" + traceback_text\n",
    "    )\n",
    "    \n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        model=\"gpt-4.1-2025-04-14\", #\"gpt-4o-mini-2024-07-18\", #\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        api_key=openai_api_key, # Make sure openai_api_key is defined\n",
    "    )\n",
    "    \n",
    "    missing_deps = [dep.strip() for dep in response.choices[0].message.content.strip().split(\"\\n\") if dep.strip() and not ' ' in dep.strip()]\n",
    "    return missing_deps\n",
    "\n",
    "async def install_missing_dependencies(container, missing_deps, repo_dir=\"/app\"):\n",
    "    \"\"\"Install missing dependencies identified from traceback\"\"\"\n",
    "    if not missing_deps:\n",
    "        return False\n",
    "    \n",
    "    # Read current requirements.txt\n",
    "    result_cat_req = await asyncio.to_thread(container.exec_run, f\"cat {repo_dir}/requirements.txt\", workdir=repo_dir)\n",
    "    current_requirements = result_cat_req.output.decode().strip().split('\\n') if result_cat_req.exit_code == 0 else []\n",
    "    \n",
    "    # Add missing dependencies that aren't already in requirements\n",
    "    new_deps = [dep for dep in missing_deps if dep not in current_requirements]\n",
    "    \n",
    "    if new_deps:\n",
    "        all_requirements = current_requirements + new_deps\n",
    "        requirements_content = \"\\n\".join(filter(None, all_requirements))  # Filter out empty strings\n",
    "        \n",
    "        # Update requirements.txt\n",
    "        await asyncio.to_thread(container.exec_run, f\"bash -c 'echo \\\"{requirements_content}\\\" > {repo_dir}/requirements.txt'\")\n",
    "        print(f\"[i] Updated requirements.txt with missing dependencies: {new_deps}\")\n",
    "        print(\"Full requirements.txt content:\", requirements_content)\n",
    "        \n",
    "        # Install the new dependencies\n",
    "        install_failed_for_any_dep = False\n",
    "        for dep in new_deps:\n",
    "            print(f\"[i] Installing missing dependency: {dep}\")\n",
    "            result_pip_install_dep = await asyncio.to_thread(container.exec_run, f\"pip install {dep}\", workdir=repo_dir)\n",
    "            print(f\"[i] Installing {dep}: {result_pip_install_dep.output.decode()}\")\n",
    "            if result_pip_install_dep.exit_code != 0:\n",
    "                install_failed_for_any_dep = True\n",
    "                # Log this specific failure if needed, though the overall run_test_code will capture the next traceback\n",
    "        return not install_failed_for_any_dep # Return True if all new deps installed successfully\n",
    "    return False\n",
    "\n",
    "async def run_test_code(container, repo_dir=\"/app\", repo_name=\"\"):\n",
    "    # Run blocking docker exec_run in a thread\n",
    "    result = await asyncio.to_thread(container.exec_run, \"python test_code.py\", workdir=repo_dir)\n",
    "    output_str = result.output.decode()\n",
    "    print(f\"[o] Output for {repo_dir} ({container.name}):\\n\", output_str)\n",
    "\n",
    "    if \"Traceback\" in output_str:#output_str.strip().startswith(\"Traceback\"):\n",
    "        print(f\"[!] Traceback detected in {container.name}. Attempting to fix missing dependencies.\")\n",
    "        \n",
    "        if repo_name in requirements_successful: # This implies initial deps (pip/npm) were okay\n",
    "            requirements_successful_code_failed[repo_name] = container.id\n",
    "        \n",
    "        dependency_install_successful_on_retry = False\n",
    "        for attempt in range(2): # Try up to 2 times to install missing Python dependencies\n",
    "            print(f\"[i] Python dependency installation attempt {attempt + 1}/2 for {repo_name}\")\n",
    "            \n",
    "            missing_deps = await extract_missing_dependencies(output_str)\n",
    "            \n",
    "            if missing_deps:\n",
    "                print(f\"[i] Found potential missing Python dependencies: {missing_deps} for {repo_name}\")\n",
    "                install_success_for_missing = await install_missing_dependencies(container, missing_deps, repo_dir)\n",
    "                \n",
    "                if install_success_for_missing:\n",
    "                    dependency_install_successful_on_retry = True\n",
    "                    print(f\"[i] Retrying test code execution in {container.name}\")\n",
    "                    retry_result = await asyncio.to_thread(container.exec_run, \"python test_code.py\", workdir=repo_dir)\n",
    "                    retry_output = retry_result.output.decode()\n",
    "                    print(f\"[o] Retry output for {repo_name} ({container.name}):\\n\", retry_output)\n",
    "                    \n",
    "                    if not \"Traceback\" in retry_output:#retry_output.strip().startswith(\"Traceback\"):\n",
    "                        print(f\"[✓] Test code succeeded after installing missing Python dependencies in {container.name}\")\n",
    "                        code_successful[repo_name] = container.id\n",
    "                        if repo_name in requirements_successful_code_failed:\n",
    "                            del requirements_successful_code_failed[repo_name]\n",
    "                        return True # Successfully fixed and run\n",
    "                    elif 'ModuleNotFoundError' not in retry_output: # Different error after installing deps\n",
    "                        append_error(repo_name, \"CODE_EXEC_ERROR_POST_FIX\", retry_output)\n",
    "                        print(f\"[!] Dependencies installed, but another error occurred in {container.name}.\")\n",
    "                        return False # Error, but not a ModuleNotFoundError\n",
    "                    else: # Still ModuleNotFoundError or other Traceback\n",
    "                        output_str = retry_output # Use new traceback for next attempt\n",
    "                else: # install_missing_dependencies returned False (failed to install one of the missing_deps)\n",
    "                    print(f\"[!] Failed to install one or more identified missing dependencies on attempt {attempt + 1} for {repo_name}.\")\n",
    "                    # No need to break, LLM might find different deps next time if traceback changes, but unlikely for same error\n",
    "            else: # No missing deps identified by LLM\n",
    "                print(f\"[!] No missing Python dependencies identified from traceback on attempt {attempt + 1} for {repo_name}.\")\n",
    "                break # Stop trying if LLM finds nothing\n",
    "        \n",
    "        # Post-loop evaluation\n",
    "        final_error_message = f\"Traceback after {attempt+1} attempts: {output_str}\"\n",
    "        if dependency_install_successful_on_retry and output_str.strip().startswith(\"Traceback\") and 'ModuleNotFoundError' in output_str:\n",
    "            # This case means we successfully installed *some* deps, but it still failed with ModuleNotFoundError\n",
    "            append_error(repo_name, \"UNRESOLVABLE_PYTHON_DEPS\", final_error_message)\n",
    "            print(f\"[!] Unable to resolve Python dependencies for {repo_name} after multiple attempts. Deleting container {container.name}.\")\n",
    "            try:\n",
    "                await asyncio.to_thread(container.stop)\n",
    "                await asyncio.to_thread(container.remove)\n",
    "                print(f\"[!] Deleted container {container.name} due to unresolvable Python dependency issues.\")\n",
    "            except docker.errors.APIError as e:\n",
    "                print(f\"[!] Error stopping/removing container {container.name}: {e}\")\n",
    "            return False\n",
    "        elif not dependency_install_successful_on_retry and output_str.strip().startswith(\"Traceback\"):\n",
    "            # This means either no deps were found, or installing them failed.\n",
    "            append_error(repo_name, \"CODE_EXEC_TRACEBACK_NO_FIX\", final_error_message)\n",
    "            print(f\"[!] Code execution failed with traceback for {repo_name}, and dependency fixes were not fully successful or not identified.\")\n",
    "            return False\n",
    "        else: # Other non-ModuleNotFoundError tracebacks, or if loop finished due to non-ModuleNotFoundError\n",
    "            append_error(repo_name, \"CODE_EXEC_ERROR_FINAL\", output_str)\n",
    "            print(f\"[!] Non-dependency error or unhandled error state in {container.name}. Keeping container for inspection.\")\n",
    "            return False\n",
    "            \n",
    "    elif \"no code found\" in output_str: # Check if the script itself says \"no code found\"\n",
    "        print(f\"[!] 'no code found' output by test_code.py in {container.name}. This implies successful execution of a placeholder.\")\n",
    "        # This could be treated as a success or a specific type of failure based on requirements.\n",
    "        # For now, let's assume it's a \"successful\" run of a script that found nothing.\n",
    "        code_successful[repo_name] = container.id \n",
    "        if repo_name in requirements_successful_code_failed:\n",
    "            del requirements_successful_code_failed[repo_name]\n",
    "        return True\n",
    "\n",
    "    elif output_str.strip().startswith(\"File not found\"): # This is a custom message, not a Python Traceback\n",
    "        append_error(repo_name, \"FILE_NOT_FOUND_ERROR\", output_str)\n",
    "        print(f\"[!] File not found error in {container.name}. Keeping container for inspection.\")\n",
    "        return False\n",
    "    \n",
    "    # Test code succeeded without issues\n",
    "    print(f\"[✓] Test code executed successfully in {container.name}.\")\n",
    "    code_successful[repo_name] = container.id\n",
    "    if repo_name in requirements_successful_code_failed:\n",
    "        del requirements_successful_code_failed[repo_name]\n",
    "    return True\n",
    "\n",
    "async def full_pipeline(git_url):\n",
    "    repo_name = git_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    \n",
    "    try:\n",
    "        repo_dir_host = await setup_repo(git_url) # Path on host\n",
    "        summary, tree, docs = await ingest_repo(repo_dir_host)\n",
    "        code = await extract_main_code(docs)\n",
    "        \n",
    "        if code.strip() == 'print(\"no code found\")':\n",
    "            print(f\"No usable Python code found by LLM for {git_url}, skipping container creation and execution.\")\n",
    "            append_error(repo_name, \"NO_USABLE_CODE_FOUND\", \"LLM returned 'print(\\\"no code found\\\")'\")\n",
    "            # Optionally, still create container if npm dependencies might be relevant for other tests later\n",
    "            # For now, we stop if no Python code.\n",
    "            return None \n",
    "            \n",
    "        write_test_code(repo_dir_host, code)\n",
    "        \n",
    "        container_name = f\"repo_{repo_name.replace('.', '_')}\" # Ensure container name is valid\n",
    "        \n",
    "        try:\n",
    "            existing_container = await asyncio.to_thread(docker_client.containers.get, container_name)\n",
    "            print(f\"Found existing container: {container_name}. Stopping and removing it.\")\n",
    "            await asyncio.to_thread(existing_container.stop)\n",
    "            await asyncio.to_thread(existing_container.remove, v=True) # v=True removes volumes associated with the container\n",
    "            print(f\"Deleted existing container: {container_name}\")\n",
    "        except docker.errors.NotFound:\n",
    "            print(f\"No existing container found with name: {container_name}. Proceeding to create a new one.\")\n",
    "        except docker.errors.APIError as e:\n",
    "            print(f\"Error managing existing container {container_name}: {e}. Attempting to proceed.\")\n",
    "            append_error(repo_name, \"DOCKER_CLEANUP_ERROR\", str(e))\n",
    "\n",
    "        repo_dir_container = \"/app\" # Standardized path inside container\n",
    "        container = await asyncio.to_thread(\n",
    "            docker_client.containers.run,\n",
    "            \"python:3.10-slim\",\n",
    "            name=container_name,\n",
    "            detach=True,\n",
    "            tty=True, # Keep STDIN open even if not attached, needed for some processes\n",
    "            volumes={str(repo_dir_host.absolute()): {'bind': repo_dir_container, 'mode': 'rw'}},\n",
    "            working_dir=repo_dir_container,\n",
    "            platform='linux/amd64'\n",
    "        )\n",
    "        print(f\"[i] Created Docker container: {container.id} ({container_name}) for {repo_name}\")\n",
    "        \n",
    "        container_info = {\"repo\": repo_name, \"container_id\": container.id, \"container_name\": container_name}\n",
    "        with open(repo_dir_host / f\"container_{repo_name}.json\", \"w\") as f:\n",
    "            json.dump(container_info, f)\n",
    "\n",
    "        dependencies_installed = await install_all_dependencies(container, docs, repo_dir_container, repo_name)\n",
    "        \n",
    "        if not dependencies_installed:\n",
    "            print(f\"[!] Dependency installation failed for {git_url}. Container {container_name} kept for inspection.\")\n",
    "            # Error already logged by install_all_dependencies\n",
    "            return container # Return container for potential manual inspection\n",
    "        \n",
    "        test_execution_successful = await run_test_code(container, repo_dir_container, repo_name)\n",
    "        \n",
    "        if not test_execution_successful:\n",
    "            print(f\"[!] Test code execution failed for {git_url}. Container {container_name} kept for inspection.\")\n",
    "            # Error logged by run_test_code\n",
    "            return container # Return container for potential manual inspection\n",
    "        \n",
    "        print(f\"[✓] Pipeline successful for {git_url}. Container ID: {container.id}\")\n",
    "        # Optionally stop/remove successful containers if not needed for inspection\n",
    "        # await asyncio.to_thread(container.stop)\n",
    "        # await asyncio.to_thread(container.remove, v=True)\n",
    "        # print(f\"[i] Stopped and removed successful container {container_name}\")\n",
    "        return container # Or return None if removing successful ones\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        append_error(repo_name, \"GIT_CLONE_ERROR\", str(e))\n",
    "        print(f\"Git clone error for {git_url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        tb_str = traceback.format_exc()\n",
    "        append_error(repo_name, \"PIPELINE_ERROR\", f\"{str(e)}\\n{tb_str}\")\n",
    "        print(f\"Pipeline error for {git_url}: {e}\\n{tb_str}\")\n",
    "        # Clean up container if it was created and an error occurred mid-pipeline\n",
    "        try:\n",
    "            if 'container' in locals() and container:\n",
    "                print(f\"Attempting to stop and remove container {container.name} due to pipeline error.\")\n",
    "                await asyncio.to_thread(container.stop)\n",
    "                await asyncio.to_thread(container.remove, v=True)\n",
    "        except Exception as cleanup_e:\n",
    "            print(f\"Error during cleanup for {container.name}: {cleanup_e}\")\n",
    "        return None\n",
    "\n",
    "async def run_all_pipelines():\n",
    "    global openai_api_key # Ensure it's accessible\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai_api_key:\n",
    "        print(\"Error: OPENAI_API_KEY environment variable not set.\")\n",
    "        return\n",
    "    openai.api_key = openai_api_key\n",
    "\n",
    "\n",
    "    successful_containers_map = {} # repo_name -> container_id for truly successful ones\n",
    "    all_created_container_ids = {} # repo_name -> container_id for all containers created (even if failed later)\n",
    "\n",
    "    Path('repos').mkdir(exist_ok=True)\n",
    "\n",
    "    for i, url in enumerate(tqdm(github_links_subset, desc=\"Processing repositories\")):\n",
    "        # time.sleep(10) # Kept from original, adjust as needed\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing repository {i+1}/{len(github_links_subset)}: {url}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        repo_name_for_tracking = url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "        \n",
    "        try:\n",
    "            container_instance = await full_pipeline(url)\n",
    "            \n",
    "            if isinstance(container_instance, docker.models.containers.Container):\n",
    "                all_created_container_ids[repo_name_for_tracking] = container_instance.id\n",
    "                # Check if it was a \"full\" success (code ran)\n",
    "                if repo_name_for_tracking in code_successful:\n",
    "                    successful_containers_map[repo_name_for_tracking] = container_instance.id\n",
    "                    print(f\"Pipeline for {url} completed successfully. Container ID: {container_instance.id}\")\n",
    "                else:\n",
    "                    print(f\"Pipeline for {url} created container {container_instance.id}, but did not complete all steps successfully.\")\n",
    "            elif container_instance is None:\n",
    "                # This case means pipeline failed before or during container creation, or container was removed.\n",
    "                print(f\"Pipeline for {url} did not result in a running container or was intentionally stopped.\")\n",
    "            else: # Should not happen\n",
    "                print(f\"Pipeline for {url} returned unexpected result: {container_instance}\")\n",
    "                append_error(repo_name_for_tracking, \"UNEXPECTED_PIPELINE_RESULT\", str(container_instance))\n",
    "                \n",
    "        except Exception as e: # Catch-all for unexpected errors in run_all_pipelines loop itself\n",
    "            import traceback\n",
    "            tb_str = traceback.format_exc()\n",
    "            append_error(repo_name_for_tracking, \"RUN_ALL_PIPELINES_LOOP_ERROR\", f\"{str(e)}\\n{tb_str}\")\n",
    "            print(f\"Outer loop exception for {url}: {e}\")\n",
    "\n",
    "        # Save tracking information periodically\n",
    "        if (i + 1) % 1 == 0 or i == len(github_links_subset) - 1: # Save every repo for better recovery\n",
    "            print(f\"\\nSaving tracking information after processing {i + 1} repositories...\")\n",
    "            \n",
    "            with open(\"repos/all_created_container_ids.json\", \"w\") as f:\n",
    "                json.dump(all_created_container_ids, f, indent=2)\n",
    "            with open(\"repos/error_log.json\", \"w\") as f:\n",
    "                json.dump(error_log, f, indent=2)\n",
    "            with open(\"repos/requirements_successful.json\", \"w\") as f:\n",
    "                json.dump(requirements_successful, f, indent=2)\n",
    "            with open(\"repos/code_successful.json\", \"w\") as f:\n",
    "                json.dump(code_successful, f, indent=2)\n",
    "            with open(\"repos/requirements_successful_code_failed.json\", \"w\") as f:\n",
    "                json.dump(requirements_successful_code_failed, f, indent=2)\n",
    "            print(\"Tracking information saved.\")\n",
    "    \n",
    "    print(f\"\\n{'='*20} Final Summary {'='*20}\")\n",
    "    print(f\"Total repositories processed: {len(github_links_subset)}\")\n",
    "    print(f\"Containers created: {len(all_created_container_ids)}\")\n",
    "    print(f\"Dependencies (pip/npm) successfully installed for: {len(requirements_successful)} repositories\")\n",
    "    print(f\"Test code executed successfully for: {len(code_successful)} repositories\")\n",
    "    print(f\"Dependencies installed but test code failed for: {len(requirements_successful_code_failed)} repositories\")\n",
    "    print(f\"Errors logged for: {len(error_log)} repositories (see repos/error_log.json)\")\n",
    "    print(f\"All created container IDs saved to repos/all_created_container_ids.json\")\n",
    "\n",
    "# Run in Jupyter/IPython:\n",
    "# Ensure OPENAI_API_KEY is set in your environment or define openai_api_key globally.\n",
    "# Example:\n",
    "# import os\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# if not openai_api_key:\n",
    "#     raise ValueError(\"OPENAI_API_KEY not set\")\n",
    "# openai.api_key = openai_api_key\n",
    "\n",
    "await run_all_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ddb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Until here the installation is complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa7225",
   "metadata": {},
   "source": [
    "### Run code inside the generated Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86841585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd Repo2Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python build_agent/main.py metabase/metabase 2d5d40a9abee16b9d4926e627d300ae764a43dbe ./repo2run/build_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b60455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp my_container:/app/output.txt ~/Desktop/output.txt #copy file from container to desktop\n",
    "# docker exec -it <container_name> bash #get into container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb257f2",
   "metadata": {},
   "source": [
    "Parallel run - Faster (first clones repos and then runs into rate limit, even for gpt4o-mini with 200k tokens/min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f270991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import openai, subprocess, os, re\n",
    "from gitingest import ingest\n",
    "import docker\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize Docker client\n",
    "docker_client = docker.from_env()\n",
    "\n",
    "# Global tracking dictionaries\n",
    "error_log = {}  # repo_name -> error_string\n",
    "requirements_successful = {}  # repo_name -> container_id\n",
    "code_successful = {}  # repo_name -> container_id\n",
    "requirements_successful_code_failed = {}  # repo_name -> container_id\n",
    "\n",
    "async def setup_repo(git_url, base_dir='repos'):\n",
    "    repo_name = git_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    print(\"Repo name is:\", repo_name)\n",
    "    repo_dir = Path(base_dir) / repo_name\n",
    "    # print(\"Repo dir is:\", repo_dir)\n",
    "    if not repo_dir.exists():\n",
    "        # Run blocking subprocess in a thread\n",
    "        await asyncio.to_thread(subprocess.run, ['git', 'clone', git_url, str(repo_dir)], check=True)\n",
    "    return repo_dir\n",
    "\n",
    "async def ingest_repo(repo_dir): # Made async\n",
    "    # Run blocking ingest call in a thread\n",
    "    # Changed include_patterns to be a set of strings as expected by gitingest\n",
    "    summary, tree, content = await asyncio.to_thread(ingest, str(repo_dir), include_patterns={\"README.md\", \"requirements.txt\"})\n",
    "    return summary, tree, content\n",
    "\n",
    "async def extract_main_code(documents): # Made async\n",
    "    prompt = (\n",
    "        \"Given the following content from a GitHub repo, extract or synthesize a minimal executable .py script \"\n",
    "        \"that demonstrates how to use the package or run its main functionality.\"\n",
    "        \"The output should be valid Python code for a `test_code.py` file. If there's nothing executable, just output \"\n",
    "        \"`print(\\\"no code found\\\")`. The content is: \"\n",
    "        f\"{documents} \\n \\n\"\n",
    "    )\n",
    "    # Use async version of OpenAI call\n",
    "    response = await openai.ChatCompletion.acreate( # Changed to acreate\n",
    "        model=\"gpt-4.1-2025-04-14\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0, \n",
    "        api_key=openai_api_key,\n",
    "    )\n",
    "    code_blocks = response.choices[0].message.content\n",
    "    # print(\"[i] Extracted code blocks:\", code_blocks)\n",
    "    return extract_first_code_block(code_blocks) or 'print(\"no code found\")'\n",
    "\n",
    "def extract_first_code_block(text):\n",
    "    m = re.search(r\"```(?:python)?\\s*(.*?)```\", text, re.DOTALL)\n",
    "    return m.group(1).strip() if m else text.strip()\n",
    "\n",
    "def write_test_code(repo_dir, code):\n",
    "    path = Path(repo_dir) / 'test_code.py'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(code)\n",
    "    return path\n",
    "\n",
    "def parse_requirements_txt(doc):\n",
    "    # Extract requirements.txt content if present\n",
    "    # Assuming 'doc' is a string containing the content of all ingested files,\n",
    "    # and each file's content is prefixed by its name.\n",
    "    # A more robust way would be if `ingest` returned structured data.\n",
    "    # For now, we search for a section starting with \"File: requirements.txt\" or similar.\n",
    "    # If `ingest` concatenates files, we need a reliable delimiter.\n",
    "    # Let's assume `gitingest` provides file content distinctly or `doc` contains identifiable sections.\n",
    "    # A simple regex for \"requirements.txt\" content block:\n",
    "    req_content = \"\"\n",
    "    # Try to find requirements.txt content. This might need adjustment based on `ingest` output format.\n",
    "    # If `docs` is a dictionary mapping filenames to content:\n",
    "    if isinstance(doc, dict) and \"requirements.txt\" in doc:\n",
    "        req_content = doc[\"requirements.txt\"]\n",
    "    # If `docs` is a single string with file contents concatenated:\n",
    "    else:\n",
    "        # This regex assumes a simple structure like \"File: requirements.txt\\n<content>\"\n",
    "        # or just the content of requirements.txt if it's the only thing related to it.\n",
    "        # This part is speculative without knowing the exact format of `docs` from `ingest`\n",
    "        # when multiple patterns are used.\n",
    "        # For gitingest, `content` is a string where files are concatenated,\n",
    "        # prefixed by \"File: <filepath>\\n---\\n<content>\\n---\".\n",
    "        match = re.search(r\"File:.*?requirements\\.txt\\n---\\n(.*?)\\n---\", doc, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            req_content = match.group(1)\n",
    "\n",
    "    requirements = []\n",
    "    if req_content:\n",
    "        for line in req_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                # Remove version specifiers, comments, and extras\n",
    "                package = re.split(r'[=<>!~#\\[]', line)[0].strip()\n",
    "                if package:\n",
    "                    requirements.append(package)\n",
    "    return requirements\n",
    "\n",
    "\n",
    "async def install_all_dependencies(container, doc, repo_dir=\"/app\", repo_name=\"\"):\n",
    "    # First get dependencies from requirements.txt if present in ingested docs\n",
    "    parsed_deps = parse_requirements_txt(doc)\n",
    "    \n",
    "    # Then get additional dependencies from content analysis using LLM\n",
    "    prompt = (\n",
    "        \"Based on the content of the file below (primarily README.md, but also consider other context if provided), \"\n",
    "        \"create a list of Python packages to be added in a requirements.txt file. \"\n",
    "        \"List only the package names, one per line. Only include packages explicitly mentioned for installation or clearly imported and used. \"\n",
    "        \"Do not include any dashes or version numbers. The content of the file is: \" + doc\n",
    "    )\n",
    "    # Use async version of OpenAI call\n",
    "    response = await openai.ChatCompletion.acreate( # Changed to acreate\n",
    "        model=\"gpt-4.1-2025-04-14\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        api_key=openai_api_key,\n",
    "    )\n",
    "    \n",
    "    llm_deps = [dep.strip() for dep in response.choices[0].message.content.strip().split(\"\\n\") if dep.strip() and not ' ' in dep.strip()]\n",
    "    \n",
    "    # Combine both sources of dependencies and remove duplicates\n",
    "    dependencies = list(dict.fromkeys(parsed_deps + llm_deps))\n",
    "    requirements_content = \"\\n\".join(dependencies)\n",
    "    \n",
    "    if not requirements_content.strip():\n",
    "        print(\"[i] No dependencies found to install.\")\n",
    "        requirements_successful[repo_name] = container.id\n",
    "        return True\n",
    "\n",
    "    # Run blocking docker exec_run in a thread\n",
    "    await asyncio.to_thread(container.exec_run, f\"bash -c 'echo \\\"{requirements_content}\\\" > {repo_dir}/requirements.txt'\")\n",
    "    print(f\"[i] Created requirements.txt in {container.name} with contents:\\n{requirements_content}\")\n",
    "    \n",
    "    # Run blocking docker exec_run in a thread\n",
    "    result = await asyncio.to_thread(container.exec_run, f\"pip install -r requirements.txt\", workdir=repo_dir)\n",
    "    print(f\"[i] Installing dependencies from requirements.txt in {repo_dir}\")\n",
    "    print(f\"[i] Output for {repo_dir}:\", result.output.decode()[:200], '\\n \\nENDING WITH:', result.output.decode()[-400:]) #only print first 200 characters\n",
    "    \n",
    "    # Check if requirements installation was successful\n",
    "    if result.exit_code == 0:\n",
    "        requirements_successful[repo_name] = container.id\n",
    "        return True\n",
    "    else:\n",
    "        error_message = f\"Requirements installation failed: {result.output.decode()}\"\n",
    "        error_log[repo_name] = error_message\n",
    "        print(f\"[!] Requirements installation failed for {repo_name}: {error_message}\")\n",
    "        return False\n",
    "\n",
    "async def extract_missing_dependencies(traceback_text):\n",
    "    \"\"\"Extract missing dependencies from traceback using OpenAI\"\"\"\n",
    "    prompt = (\n",
    "        \"Given the following Python traceback error, identify any missing Python packages that need to be installed. \"\n",
    "        \"Return only the package names, one per line, without any version numbers or additional text. \"\n",
    "        \"If no missing packages can be identified, return an empty response. \"\n",
    "        \"The traceback is: \" + traceback_text\n",
    "    )\n",
    "    \n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        model=\"gpt-4.1-2025-04-14\", #\"gpt-4o-mini-2024-07-18\", #\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        api_key=openai_api_key,\n",
    "    )\n",
    "    \n",
    "    missing_deps = [dep.strip() for dep in response.choices[0].message.content.strip().split(\"\\n\") if dep.strip() and not ' ' in dep.strip()]\n",
    "    return missing_deps\n",
    "\n",
    "async def install_missing_dependencies(container, missing_deps, repo_dir=\"/app\"):\n",
    "    \"\"\"Install missing dependencies identified from traceback\"\"\"\n",
    "    if not missing_deps:\n",
    "        return False\n",
    "    \n",
    "    # Read current requirements.txt\n",
    "    result = await asyncio.to_thread(container.exec_run, f\"cat {repo_dir}/requirements.txt\", workdir=repo_dir)\n",
    "    current_requirements = result.output.decode().strip().split('\\n') if result.exit_code == 0 else []\n",
    "    \n",
    "    # Add missing dependencies that aren't already in requirements\n",
    "    new_deps = [dep for dep in missing_deps if dep not in current_requirements]\n",
    "    \n",
    "    if new_deps:\n",
    "        all_requirements = current_requirements + new_deps\n",
    "        requirements_content = \"\\n\".join(filter(None, all_requirements))  # Filter out empty strings\n",
    "        \n",
    "        # Update requirements.txt\n",
    "        await asyncio.to_thread(container.exec_run, f\"bash -c 'echo \\\"{requirements_content}\\\" > {repo_dir}/requirements.txt'\")\n",
    "        print(f\"[i] Updated requirements.txt with missing dependencies: {new_deps}\")\n",
    "        print(\"Full requirements.txt content:\", requirements_content)\n",
    "        \n",
    "        # Install the new dependencies\n",
    "        for dep in new_deps:\n",
    "            result = await asyncio.to_thread(container.exec_run, f\"pip install {dep}\", workdir=repo_dir)\n",
    "            print(f\"[i] Installing {dep}: {result.output.decode()}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "async def run_test_code(container, repo_dir=\"/app\", repo_name=\"\"):\n",
    "    # Run blocking docker exec_run in a thread\n",
    "    result = await asyncio.to_thread(container.exec_run, \"python test_code.py\", workdir=repo_dir)\n",
    "    output_str = result.output.decode()\n",
    "    print(f\"[o] Output for {repo_dir} ({container.name}):\\n\", output_str)\n",
    "\n",
    "    if output_str.strip().startswith(\"Traceback\"):\n",
    "        print(f\"[!] Traceback detected in {container.name}. Attempting to fix missing dependencies.\")\n",
    "        \n",
    "        # Track that requirements were successful but code initially failed\n",
    "        if repo_name in requirements_successful:\n",
    "            requirements_successful_code_failed[repo_name] = container.id\n",
    "        \n",
    "        # Try up to 2 times to install missing dependencies\n",
    "        dependency_install_successful = False\n",
    "        for attempt in range(2):\n",
    "            print(f\"[i] Dependency installation attempt {attempt + 1}/2\")\n",
    "            \n",
    "            # Extract missing dependencies from the traceback\n",
    "            missing_deps = await extract_missing_dependencies(output_str)\n",
    "            \n",
    "            if missing_deps:\n",
    "                print(f\"[i] Found potential missing dependencies: {missing_deps}\")\n",
    "                install_success = await install_missing_dependencies(container, missing_deps, repo_dir)\n",
    "                \n",
    "                if install_success:\n",
    "                    dependency_install_successful = True\n",
    "                    # Retry running the test code\n",
    "                    print(f\"[i] Retrying test code execution in {container.name}\")\n",
    "                    retry_result = await asyncio.to_thread(container.exec_run, \"python test_code.py\", workdir=repo_dir)\n",
    "                    retry_output = retry_result.output.decode()\n",
    "                    print(f\"[o] Retry output for ({container.name}):\\n\", retry_output)\n",
    "                    \n",
    "                    # Check if retry was successful\n",
    "                    if not retry_output.strip().startswith(\"Traceback\"):\n",
    "                        print(f\"[✓] Test code succeeded after installing missing dependencies in {container.name}\")\n",
    "                        code_successful[repo_name] = container.id\n",
    "                        # Remove from failed dict if it was there\n",
    "                        if repo_name in requirements_successful_code_failed:\n",
    "                            del requirements_successful_code_failed[repo_name]\n",
    "                        return True\n",
    "                    elif 'ModuleNotFoundError' not in retry_output:\n",
    "                        print(f\"[!] Dependencies installed successfully in {container.name} but other error occurred\")\n",
    "                        error_log[repo_name] = f\"Code execution error after dependency installation: {retry_output}\"\n",
    "                        return False\n",
    "                    else:\n",
    "                        # Update output_str for next iteration if there's another attempt\n",
    "                        output_str = retry_output\n",
    "            else:\n",
    "                print(f\"[!] No missing dependencies identified from traceback on attempt {attempt + 1}.\")\n",
    "                break  # No deps found, don't continue trying\n",
    "        \n",
    "        # If we've exhausted attempts and still have dependency issues, delete container\n",
    "        if dependency_install_successful and output_str.strip().startswith(\"Traceback\") and 'ModuleNotFoundError' in output_str:\n",
    "            print(f\"[!] Unable to resolve dependencies after 2 attempts. Deleting container {container.name}.\")\n",
    "            error_log[repo_name] = f\"Unresolvable dependency issues after 2 attempts: {output_str}\"\n",
    "            try:\n",
    "                await asyncio.to_thread(container.stop)\n",
    "                await asyncio.to_thread(container.remove)\n",
    "                print(f\"[!] Deleted container {container.name} due to unresolvable dependency issues.\")\n",
    "            except docker.errors.APIError as e:\n",
    "                print(f\"[!] Error stopping/removing container {container.name}: {e}\")\n",
    "            return False\n",
    "        elif not dependency_install_successful:\n",
    "            error_log[repo_name] = f\"No dependencies could be extracted from traceback: {output_str}\"\n",
    "            print(f\"[!] No dependencies could be extracted. Keeping container {container.name} for manual inspection.\")\n",
    "            return False\n",
    "        else:\n",
    "            error_log[repo_name] = f\"Code execution error (non-dependency): {output_str}\"\n",
    "            print(f\"[!] Non-dependency error in {container.name}. Keeping container for inspection.\")\n",
    "            return False\n",
    "            \n",
    "    elif output_str.strip().startswith(\"File not found\"):\n",
    "        print(f\"[!] File not found error in {container.name}. Keeping container for inspection.\")\n",
    "        error_log[repo_name] = f\"File not found error: {output_str}\"\n",
    "        return False\n",
    "    \n",
    "    # Test code succeeded\n",
    "    code_successful[repo_name] = container.id\n",
    "    # Remove from failed dict if it was there\n",
    "    if repo_name in requirements_successful_code_failed:\n",
    "        del requirements_successful_code_failed[repo_name]\n",
    "    return True\n",
    "\n",
    "async def full_pipeline(git_url):\n",
    "    repo_name = git_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    \n",
    "    try:\n",
    "        repo_dir = await setup_repo(git_url)\n",
    "        summary, tree, docs = await ingest_repo(repo_dir) # Added await\n",
    "        # print(f\"[i] Ingested documents for {git_url}:\\n{docs}\\n\") # For debugging doc format\n",
    "        # print(\"[i] Summary:\", summary)\n",
    "        # print(\"[i] Tree:\", tree)\n",
    "        code = await extract_main_code(docs) # Added await\n",
    "        \n",
    "        # If no code found, return early without creating container\n",
    "        if code.strip() == 'print(\"no code found\")':\n",
    "            print(f\"No usable code found for {git_url}, skipping container creation\")\n",
    "            error_log[repo_name] = \"No usable code found\"\n",
    "            return None\n",
    "            \n",
    "        write_test_code(repo_dir, code) # Sync, assumed fast\n",
    "        \n",
    "        container_name = f\"repo_{repo_name}\"\n",
    "        \n",
    "        # Check if container already exists and delete it\n",
    "        try:\n",
    "            existing_container = await asyncio.to_thread(docker_client.containers.get, container_name)\n",
    "            if existing_container:\n",
    "                print(f\"Found existing container with name: {container_name}. Deleting it.\")\n",
    "                await asyncio.to_thread(existing_container.stop)\n",
    "                await asyncio.to_thread(existing_container.remove)\n",
    "                print(f\"Deleted container with name: {container_name}\")\n",
    "        except docker.errors.NotFound:\n",
    "            print(f\"No existing container found with name: {container_name}. Proceeding to create a new one.\")\n",
    "        except docker.errors.APIError as e:\n",
    "            print(f\"Error checking for existing container {container_name}: {e}\")\n",
    "            # Decide if you want to stop or continue. For now, let's try to continue.\n",
    "            pass\n",
    "\n",
    "        # Run blocking docker containers.run in a thread using Linux-based Python image\n",
    "        container = await asyncio.to_thread(\n",
    "            docker_client.containers.run,\n",
    "            \"python:3.10-slim\",\n",
    "            name=container_name,\n",
    "            detach=True,\n",
    "            tty=True,\n",
    "            volumes={str(repo_dir.absolute()): {'bind': '/app', 'mode': 'rw'}},\n",
    "            working_dir='/app',\n",
    "            platform='linux/amd64' # Specify platform for consistency\n",
    "        )\n",
    "        \n",
    "        print(f\"[i] Created Docker container: {container_name}\")\n",
    "        \n",
    "        container_info = {\n",
    "            \"repo\": repo_name,\n",
    "            \"container_id\": container.id,\n",
    "            \"container_name\": container_name\n",
    "        }\n",
    "        \n",
    "        # Save json inside repo directory\n",
    "        with open(repo_dir / f\"container_{repo_name}.json\", \"w\") as f:\n",
    "            json.dump(container_info, f)\n",
    "\n",
    "        # Install dependencies only after container is confirmed running\n",
    "        requirements_success = await install_all_dependencies(container, docs, \"/app\", repo_name)\n",
    "        \n",
    "        if not requirements_success:\n",
    "            print(f\"Requirements installation failed for {git_url}. Keeping container {container_name} for inspection.\")\n",
    "            return container\n",
    "        \n",
    "        test_successful = await run_test_code(container, \"/app\", repo_name)\n",
    "        \n",
    "        return container\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_log[repo_name] = f\"Pipeline error: {str(e)}\"\n",
    "        print(f\"Pipeline error for {git_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def run_all_pipelines():\n",
    "    # Ensure openai_api_key is set before running, e.g.\n",
    "    # openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    # Or ensure it's globally available if not passed explicitly to functions.\n",
    "    # For this example, I've modified extract_main_code and install_all_dependencies\n",
    "    # to use os.getenv(\"OPENAI_API_KEY\") directly.\n",
    "\n",
    "    # Ensure repos directory exists early\n",
    "    Path('repos').mkdir(exist_ok=True)\n",
    "\n",
    "    tasks = [full_pipeline(url) for url in github_links_subset]\n",
    "    # All tasks are launched and awaited together, maintaining parallel execution.\n",
    "    containers_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    successful_containers = []\n",
    "    # container_ids will be populated as we process results.\n",
    "    # It should be a local variable, not assumed global for population here.\n",
    "    container_ids = {} \n",
    "\n",
    "    # Helper function to save all tracking information\n",
    "    def _save_all_logs():\n",
    "        # Save all tracking information\n",
    "        with open(\"repos/all_container_ids.json\", \"w\") as f:\n",
    "            json.dump(container_ids, f, indent=2)\n",
    "        \n",
    "        # These global dictionaries are assumed to be populated by full_pipeline\n",
    "        with open(\"repos/error_log.json\", \"w\") as f:\n",
    "            json.dump(error_log, f, indent=2)\n",
    "        \n",
    "        with open(\"repos/requirements_successful.json\", \"w\") as f:\n",
    "            json.dump(requirements_successful, f, indent=2)\n",
    "        \n",
    "        with open(\"repos/code_successful.json\", \"w\") as f:\n",
    "            json.dump(code_successful, f, indent=2)\n",
    "        \n",
    "        with open(\"repos/requirements_successful_code_failed.json\", \"w\") as f:\n",
    "            json.dump(requirements_successful_code_failed, f, indent=2)\n",
    "        \n",
    "        print(\"Successfully saved tracking information to 'repos/' directory.\")\n",
    "\n",
    "    for i, result in enumerate(containers_results):\n",
    "        url = github_links_subset[i]\n",
    "        # Assuming repo_name is derived consistently for logging keys\n",
    "        repo_name_for_id = url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "\n",
    "        if isinstance(result, docker.models.containers.Container):\n",
    "            successful_containers.append(result)\n",
    "            container_ids[repo_name_for_id] = result.id\n",
    "            print(f\"Pipeline for {url} completed. Container ID: {result.id}\")\n",
    "        elif result is None:\n",
    "            # This case is handled if full_pipeline returns None (e.g. test code failed or other handled error)\n",
    "            # error_log and other relevant logs should have been populated by full_pipeline.\n",
    "            print(f\"Pipeline for {url} did not complete successfully or container was removed.\")\n",
    "        elif isinstance(result, Exception):\n",
    "            # This means asyncio.gather caught an unhandled exception from full_pipeline.\n",
    "            # full_pipeline is expected to log its own errors to error_log.\n",
    "            # This print is for visibility of exceptions that might not have been caught by full_pipeline's own try-except.\n",
    "            print(f\"Pipeline for {url} failed with an exception: {result}\")\n",
    "            # Optionally, log the traceback:\n",
    "            # import traceback\n",
    "            # traceback.print_exception(type(result), result, result.__traceback__)\n",
    "            # If full_pipeline doesn't guarantee logging for all its exceptions,\n",
    "            # you might want to add to error_log here:\n",
    "            # if repo_name_for_id not in error_log: # Avoid overwriting more specific error from full_pipeline\n",
    "            #     error_log[repo_name_for_id] = f\"Unhandled pipeline error: {str(result)}\"\n",
    "\n",
    "        # Save logs every 5 iterations (except for the very last one, which is covered by final save)\n",
    "        if (i + 1) % 5 == 0 and (i + 1) < len(containers_results):\n",
    "            print(f\"Processed {i + 1}/{len(containers_results)} repositories. Saving intermediate logs...\")\n",
    "            _save_all_logs()\n",
    "            # Print intermediate summary counts\n",
    "            print(f\"  So far: {len(successful_containers)} successful containers.\")\n",
    "            print(f\"  So far: {len(requirements_successful)} requirements successful.\")\n",
    "            print(f\"  So far: {len(code_successful)} code successful.\")\n",
    "            print(f\"  So far: {len(requirements_successful_code_failed)} reqs successful, code failed.\")\n",
    "            print(f\"  So far: {len(error_log)} errors logged.\")\n",
    "\n",
    "\n",
    "    # Final save of all logs after processing all results\n",
    "    print(f\"Finished processing all {len(containers_results)} repositories. Saving final logs...\")\n",
    "    _save_all_logs()\n",
    "    \n",
    "    # Final summary prints\n",
    "    print(f\"Successfully processed {len(successful_containers)} repositories.\")\n",
    "    print(f\"Requirements successful: {len(requirements_successful)} repositories\")\n",
    "    print(f\"Code successful: {len(code_successful)} repositories\")\n",
    "    print(f\"Requirements successful but code failed: {len(requirements_successful_code_failed)} repositories\")\n",
    "    print(f\"Errors logged: {len(error_log)} repositories\")\n",
    "    \n",
    "    print(f\"Container IDs saved to repos/all_container_ids.json: {container_ids}\")\n",
    "    print(\"Error log saved to repos/error_log.json\")\n",
    "    print(\"Requirements successful saved to repos/requirements_successful.json\")\n",
    "    print(\"Code successful saved to repos/code_successful.json\")\n",
    "    print(\"Requirements successful but code failed saved to repos/requirements_successful_code_failed.json\")\n",
    "\n",
    "# Run in Jupyter/IPython:\n",
    "await run_all_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run the above to get into the container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f28df7",
   "metadata": {},
   "source": [
    "Run test code inside a container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "# Create the Python script content\n",
    "fireducks_code = '''import fireducks.pandas as pd\n",
    "\n",
    "def main():\n",
    "    # Create a simple DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"A\": [1, 30, 3],\n",
    "        \"B\": [4, 5, 6]\n",
    "    })\n",
    "    print(\"FireDucks DataFrame:\")\n",
    "    print(df)\n",
    "    # Perform a simple operation\n",
    "    print(\"\\\\nSum of column A:\", df[\"A\"].sum())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Minimal debugging for containers and run the test files\n",
    "git_urls = [\n",
    "    \"https://github.com/fireducks-dev/fireducks/\",\n",
    "    \"https://github.com/astral-sh/uv\"\n",
    "]\n",
    "\n",
    "client = docker.from_env()\n",
    "\n",
    "for git_url in git_urls:\n",
    "    repo_name = git_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    container_info_file = f\"container_{repo_name}.json\"\n",
    "    \n",
    "    print(f\"\\n=== Debugging and Running {repo_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        with open(container_info_file) as f:\n",
    "            container_data = json.load(f)\n",
    "        \n",
    "        container_id = container_data.get('container_id')\n",
    "        print(f\"Container ID: {container_id}\")\n",
    "        \n",
    "        if container_id:\n",
    "            try:\n",
    "                container = client.containers.get(container_id)\n",
    "                print(f\"Container status: {container.status}\")\n",
    "                print(f\"Container name: {container.name}\")\n",
    "                \n",
    "                # Simple test command\n",
    "                result = container.exec_run(\"echo 'Container is responsive'\")\n",
    "                print(f\"Test command output: {result.output.decode().strip()}\")\n",
    "                print(f\"Test command exit code: {result.exit_code}\")\n",
    "                \n",
    "                # Run the test files in the workdir app\n",
    "                if repo_name == \"fireducks\":\n",
    "                    print(f\"\\n--- Running test_fireducks for {repo_name} ---\")\n",
    "                    result = container.exec_run(\"python test_fireducks.py\")\n",
    "                    print(f\"test_fireducks output:\\n{result.output.decode().strip()}\")\n",
    "                    print(f\"test_fireducks exit code: {result.exit_code}\")\n",
    "                    \n",
    "                    # Also run the fireducks_code variable\n",
    "                    print(f\"\\n--- Running fireducks_code variable for {repo_name} ---\")\n",
    "                    try:\n",
    "                        # Use single quotes for the -c argument to avoid issues with double quotes in fireducks_code\n",
    "                        command = f\"python -c '{fireducks_code}'\"\n",
    "                        result = container.exec_run(command) # Re-assign result, matches existing pattern\n",
    "                        \n",
    "                        output_str = result.output.decode('utf-8', errors='replace').strip()\n",
    "                        exit_code = result.exit_code\n",
    "\n",
    "                        print(f\"fireducks_code output:\\n{output_str}\")\n",
    "                        print(f\"fireducks_code exit code: {exit_code}\")\n",
    "                        \n",
    "                    except docker.errors.APIError as api_e:\n",
    "                        print(f\"Docker API error during fireducks_code execution for {repo_name}: {api_e}\")\n",
    "                        traceback.print_exc()\n",
    "                    except Exception as exec_e:\n",
    "                        print(f\"Error during fireducks_code execution for {repo_name}: {exec_e}\")\n",
    "                        traceback.print_exc()\n",
    "                        \n",
    "                elif repo_name == \"uv\":\n",
    "                    print(f\"\\n--- Running test for {repo_name} ---\")\n",
    "                    result = container.exec_run(\"python test_uv.py\")\n",
    "                    print(f\"test output:\\n{result.output.decode().strip()}\")\n",
    "                    print(f\"test exit code: {result.exit_code}\")\n",
    "                \n",
    "            except docker.errors.NotFound:\n",
    "                print(f\"Container {container_id} not found\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing container: {e}\")\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(\"No container_id found\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Container info file {container_info_file} not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c7f0a",
   "metadata": {},
   "source": [
    "### Sandboxes - Not persistent, have to rerun once restart session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00720dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import openai, subprocess, os, re\n",
    "from gitingest import ingest\n",
    "from e2b_code_interpreter import Sandbox\n",
    "from pathlib import Path\n",
    "\n",
    "async def setup_repo(git_url, base_dir='sandboxes'):\n",
    "    repo_name = git_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    print(\"Repo name is:\", repo_name)\n",
    "    sandbox_dir = Path(base_dir) / repo_name\n",
    "    print(\"Sandbox dir is:\", sandbox_dir)\n",
    "    if not sandbox_dir.exists():\n",
    "        subprocess.run(['git', 'clone', git_url, str(sandbox_dir)], check=True)\n",
    "    return sandbox_dir\n",
    "\n",
    "def ingest_repo(sandbox_dir):\n",
    "    summary, tree, content = ingest(str(sandbox_dir), include_patterns=\"README.md\")\n",
    "    return summary, tree, content\n",
    "\n",
    "def extract_main_code(documents):\n",
    "    prompt = (\n",
    "        \"Given the following content from a GitHub repo, extract or synthesize a minimal executable .py script \"\n",
    "        \"that demonstrates how to use the package or run its main functionality.\"\n",
    "        \"The output should be valid Python code for a `main.py` file. If there's nothing executable, just output \"\n",
    "        \"`print(\\\"no code found\\\")`. The content is: \"\n",
    "        f\"{documents} \\n \\n\"\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4.1-2025-04-14\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0, \n",
    "        api_key=openai_api_key,\n",
    "    )\n",
    "    code_blocks = response.choices[0].message.content\n",
    "    print(\"[i] Extracted code blocks:\", code_blocks)\n",
    "    return extract_first_code_block(code_blocks) or 'print(\"no code found\")'\n",
    "\n",
    "def extract_first_code_block(text):\n",
    "    m = re.search(r\"```(?:python)?\\s*(.*?)```\", text, re.DOTALL)\n",
    "    return m.group(1).strip() if m else text.strip()\n",
    "\n",
    "def write_main_py(sandbox_dir, code):\n",
    "    path = Path(sandbox_dir) / 'main.py'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(code)\n",
    "    return path\n",
    "\n",
    "async def install_all_dependencies(sandbox, doc, repo_dir=\"/\"):\n",
    "    prompt = (\n",
    "        \"Based on the content of the file below, create a list of python packages to be added in a requirements.txt file.\"\n",
    "        \"List only the package names, one per line. Only include mentioned to be installed in the content of the file. The content of the file is: \" + doc\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4.1-2025-04-14\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        api_key=openai_api_key,\n",
    "    )\n",
    "    \n",
    "    # Create requirements.txt from OpenAI response\n",
    "    dependencies = response.choices[0].message.content.strip().split(\"\\n\")\n",
    "    requirements_content = \"\\n\".join(dependencies)\n",
    "    \n",
    "    # Write requirements.txt to sandbox\n",
    "    sandbox.files.write(\"/requirements.txt\", requirements_content)\n",
    "    print(\"[i] Created requirements.txt with contents:\\n\", requirements_content)\n",
    "    \n",
    "    # Install dependencies\n",
    "    result = sandbox.commands.run(\"pip install -r requirements.txt\", cwd=repo_dir)\n",
    "    print(\"[i] Installing dependencies from requirements.txt\")\n",
    "    print(\"[i] STDOUT:\", result.stdout)\n",
    "    print(\"[i] STDERR:\", result.stderr or \"(None)\")\n",
    "\n",
    "    # if \"pyproject.toml\" in file_names:\n",
    "    #     result = sandbox.commands.run(\"pip install .\", cwd=repo_dir)\n",
    "    #     print(\"[i] Installing package from pyproject.toml\")\n",
    "    #     print(\"[i] STDOUT:\", result.stdout)\n",
    "    #     print(\"[i] STDERR:\", result.stderr or \"(None)\")\n",
    "\n",
    "async def run_main(sandbox, repo_dir):\n",
    "    result = sandbox.commands.run(\"python main.py\", cwd=repo_dir)\n",
    "    print(\"[o] Output:\\n\", result.stdout)\n",
    "    print(\"[o] Errors:\\n\", result.stderr)\n",
    "\n",
    "async def full_pipeline(git_url):\n",
    "    repo_dir = await setup_repo(git_url)\n",
    "    summary, tree, docs = ingest_repo(repo_dir)\n",
    "    print(\"[i] Ingested documents:\", len(docs), docs, \"\\n \\n\")\n",
    "    print(\"[i] Summary:\", summary)\n",
    "    print(\"[i] Tree:\", tree)\n",
    "    code = extract_main_code(docs)\n",
    "    write_main_py(repo_dir, code)\n",
    "    \n",
    "    # Create sandbox\n",
    "    sb = Sandbox(template=\"base\")\n",
    "    \n",
    "    # Save sandbox ID to file\n",
    "    sandbox_id = sb.sandbox_id\n",
    "    repo_name = git_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    sandbox_info = {\n",
    "        \"repo\": repo_name,\n",
    "        \"sandbox_id\": sandbox_id\n",
    "    }\n",
    "    \n",
    "    with open(f\"sandbox_{repo_name}.json\", \"w\") as f:\n",
    "        json.dump(sandbox_info, f)\n",
    "\n",
    "    # Upload files to sandbox\n",
    "    for file in Path(repo_dir).rglob(\"*\"):\n",
    "        if file.is_file():\n",
    "            rel_path = file.relative_to(repo_dir).as_posix()\n",
    "            with open(file, \"rb\") as f:\n",
    "                content = f.read()\n",
    "            sb.files.write(f\"/{rel_path}\", content)\n",
    "\n",
    "    await install_all_dependencies(sb, docs, \"/\")\n",
    "    await run_main(sb, \"/\")\n",
    " # To run this in a Jupyter or IPython environment:\n",
    "# await full_pipeline(\"https://github.com/fireducks-dev/fireducks/\")   \n",
    "    return sb\n",
    "\n",
    "# Git URLs to process\n",
    "git_urls = [\n",
    "    \"https://github.com/fireducks-dev/fireducks/\",\n",
    "    \"https://github.com/astral-sh/uv\"\n",
    "    # Add more git URLs here\n",
    "]\n",
    "\n",
    "async def run_all_pipelines():\n",
    "    tasks = [full_pipeline(url) for url in git_urls]\n",
    "    sandboxes = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Save all sandbox IDs to a single file\n",
    "    sandbox_ids = {\n",
    "        url.rstrip('/').split('/')[-1].replace('.git', ''): sb.sandbox_id \n",
    "        for url, sb in zip(git_urls, sandboxes)\n",
    "    }\n",
    "    \n",
    "    with open(\"all_sandbox_ids.json\", \"w\") as f:\n",
    "        json.dump(sandbox_ids, f)\n",
    "\n",
    "# Run in Jupyter/IPython:\n",
    "await run_all_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2b_code_interpreter import Sandbox\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Git URLs to process\n",
    "git_urls = [\n",
    "    \"https://github.com/fireducks-dev/fireducks/\",\n",
    "    \"https://github.com/astral-sh/uv\"\n",
    "    # Add more git URLs here\n",
    "]\n",
    "\n",
    "# Load sandboxes based on git URLs - each is saved in a sandbox_name.json file\n",
    "sandboxes = {}\n",
    "\n",
    "for git_url in git_urls:\n",
    "    # Extract repo name from git URL\n",
    "    repo_name = git_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    sandbox_file = f\"sandbox_{repo_name}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(sandbox_file) as f:\n",
    "            sandbox_info = json.load(f)\n",
    "        print(f\"Sandbox info: {sandbox_info}, type: {type(sandbox_info)}\")\n",
    "        # Connect to the sandbox using the ID from the file\n",
    "        sandbox_id = sandbox_info.get('sandbox_id')\n",
    "        print(f\"Sandbox ID: {sandbox_id}\")\n",
    "        print(f\"Repo name: {repo_name}\")\n",
    "        if sandbox_id:\n",
    "            # print(f\"Connecting to sandbox for {Sandbox.connect(sandbox_id)}\")\n",
    "            sandbox = Sandbox.connect(sandbox_id=sandbox_id, api_key=os.getenv('E2B_API_KEY'))\n",
    "\n",
    "            sandboxes[repo_name] = sandbox#Sandbox.connect(sandbox_id)\n",
    "            print(f\"Connected to sandbox for {repo_name}: {sandbox_id}\")\n",
    "            # If sandbox has 'fire' in its name, run the test_fireducks code\n",
    "            if 'fire' in repo_name.lower():\n",
    "                print(f\"Running test_fireducks for {repo_name}...\")\n",
    "                result = sandboxes[repo_name].commands.run(\"python test_fireducks.py\", cwd=f\"/sandboxes/{repo_name}\")\n",
    "                print(f\"Test output:\\n{result.stdout}\")\n",
    "                if result.stderr:\n",
    "                    print(f\"Test errors:\\n{result.stderr}\")\n",
    "        else:\n",
    "            print(f\"No sandbox_id found in {sandbox_file}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Sandbox file {sandbox_file} not found for {repo_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sandbox for {repo_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nLoaded {len(sandboxes)} sandboxes:\")\n",
    "for repo_name in sandboxes.keys():\n",
    "    print(f\"- {repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15462ba9",
   "metadata": {},
   "source": [
    "Example of retrieving and running code inside a sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6063e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2b_code_interpreter import Sandbox\n",
    "import json\n",
    "\n",
    "# Load IDs from the correct file (all_sandbox_ids.json was saved by run_all_pipelines)\n",
    "with open(\"all_sandbox_ids.json\") as f:\n",
    "    sandbox_ids = json.load(f)\n",
    "\n",
    "# Reconnect to all sandboxes using correct syntax\n",
    "sandboxes = {}\n",
    "for repo_name, sandbox_id in sandbox_ids.items():\n",
    "    sandboxes[repo_name] = Sandbox.connect(sandbox_id)\n",
    "\n",
    "# Now you can use them - example with fireducks\n",
    "fireducks_sb = sandboxes.get(\"fireducks\")\n",
    "if fireducks_sb:\n",
    "    # Test basic Python\n",
    "    result = fireducks_sb.commands.run(\"python --version\")\n",
    "    print(f\"Python version: {result.stdout}\")\n",
    "    \n",
    "    # Test fireducks import and basic usage\n",
    "    fireducks_sb.files.write(\"test_fireducks.py\", \"\"\"\n",
    "import fireducks.pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a simple DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50],\n",
    "    'C': ['a', 'b', 'c', 'd', 'e']\n",
    "})\n",
    "\n",
    "print(\"DataFrame created:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\\\nDataFrame info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\\\nDataFrame describe:\")\n",
    "print(df.describe())\n",
    "\"\"\")\n",
    "    \n",
    "    result = fireducks_sb.commands.run(\"python test_fireducks.py\")\n",
    "    print(f\"Fireducks test output:\\n{result.stdout}\")\n",
    "    if result.stderr:\n",
    "        print(f\"Fireducks test errors:\\n{result.stderr}\")\n",
    "\n",
    "# You can also iterate through all sandboxes\n",
    "print(\"\\nAvailable sandboxes:\")\n",
    "for repo_name, sb in sandboxes.items():\n",
    "    print(f\"- {repo_name}: {sb.sandbox_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_git",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
